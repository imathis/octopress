<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Ruby | Avi Tzurel]]></title>
  <link href="http://avi.io/blog/categories/ruby/atom.xml" rel="self"/>
  <link href="http://avi.io/"/>
  <updated>2013-12-03T23:01:13+02:00</updated>
  <id>http://avi.io/</id>
  <author>
    <name><![CDATA[Avi Tzurel]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Upload folder to S3 recursively]]></title>
    <link href="http://avi.io/blog/2013/12/03/upload-folder-to-s3-recursively/"/>
    <updated>2013-12-03T19:46:00+02:00</updated>
    <id>http://avi.io/blog/2013/12/03/upload-folder-to-s3-recursively</id>
    <content type="html"><![CDATA[<p>Lately, I needed to upload a folder to S3 with all of it's files.</p>

<p>The use-case is compiling assets on the CI and then uploading it to S3 for the CDN to consume.</p>

<p>While searching for a gem that does it I encountered <a href="https://github.com/chrishein/s3_uploader">s3_uploader</a>, but I really didn't like it because it's using Fog.</p>

<p>Generally, I don't like gems that use other gems for no apparent reason, there's absolutely no reason to include fog in my project just to upload files recursively.</p>

<p>I did however, like that it's using multi threads in order to do the upload so I am doing the same in my solution.</p>

<p>I wrote a solution that uses the aws-s3 Ruby SDK, which was already included in my project anyway.</p>

<p>Here's the code:</p>

<p>```ruby</p>

<pre><code>class S3FolderUpload
  attr_reader :folder_path, :total_files, :bucket, :s3_connection
  attr_accessor :files

  # Initialize the upload class
  #
  # folder_path - path to the folder that you want to upload
  # bucket - The bucket you want to upload to
  # aws_key - Your key generated by AWS defaults to the environemt setting AWS_KEY_ID
  # aws_secret - The secret generated by AWS
  #
  # Examples
  #   =&gt; uploader = S3FolderUpload.new("some_route/test_folder", 'your_bucket_name')
  #
  def initialize(folder_path, bucket, aws_key = ENV['AWS_KEY_ID'], aws_secret = ENV['AWS_SECRET'])
    @folder_path       = folder_path
    @bucket            = bucket
    @files             = Dir.glob("#{folder_path}/**/*")
    @total_files       = files.length
    @s3_connection     = AWS::S3.new(:access_key_id =&gt; aws_key,
                                    :secret_access_key =&gt; aws_secret)
  end

  # public: Upload files from the folder to S3
  #
  # thread_count - How many threads you want to use (defaults to 5)
  #
  # Examples
  #   =&gt; uploader.upload!(20)
  #     true
  #   =&gt; uploader.upload!
  #     true
  #
  # Returns true when finished the process
  def upload!(thread_count = 5)
    file_number = 0
    mutex       = Mutex.new
    threads     = []

    thread_count.times do |i|
      threads[i] = Thread.new {
        until files.empty?
          mutex.synchronize do
            file_number += 1
            Thread.current["file_number"] = file_number
          end
          file = files.pop rescue nil
          next unless file

          # I had some more manipulation here figuring out the git sha
          # For the sake of the example, we'll leave it simple
          #
          path = file

          puts "[#{Thread.current["file_number"]}/#{total_files}] uploading..."
          data = File.open(file)
          next if File.directory? data

          puts "Path: #{path}"
          obj = s3_connection.buckets[bucket].objects[path]
          obj.write(data, {})
        end
      }
    end
    threads.each { |t| t.join }
  end
end
</code></pre>

<p>```</p>

<p>The usage is really simple</p>

<p>```ruby</p>

<pre><code>uploader = S3FolderUpload.new('folder_name', 'your_bucket', aws_key, aws_secret)
uploader.upload!
</code></pre>

<p>```</p>

<p>Since it's using Threads, the upload is really fast, both from local machines and from servers.</p>

<p>Have fun coding!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Real life usage for mongoexport (and a bonus)]]></title>
    <link href="http://avi.io/blog/2013/11/22/export-mongo-collection-to-json-with-query/"/>
    <updated>2013-11-22T16:39:00+02:00</updated>
    <id>http://avi.io/blog/2013/11/22/export-mongo-collection-to-json-with-query</id>
    <content type="html"><![CDATA[<p>I work a lot with MongoDB, and this is not the "Why not MongoDB" or "Why MongoDB" type of post, I know it's popular to trash Mongo lately.</p>

<p>Anyway, while working with Mongo, from time to time I need to export a collection, or a subset of the collection based on a query.</p>

<p>I love using <code>mongoexport</code> because you basically get JSON file out of it (or CSV) and from there on you can pretty much do anything you want with it.</p>

<p>You can use Amazon's MapReduce or any other solution you may want.</p>

<p>For example, when I do usually is export a list I need, load it into Rails console and work with the output, queue it up to the worker list etc...</p>

<p>Let's cover some scenarios I use the most</p>

<h2>Export Collection To JSON</h2>

<p>Actually, I never use it, since the collections are too big for the disk to handle at once, we have a sharded collection, so no one-disk solution can hold the data.</p>

<p>That been said, I think for most people this can be very useful.</p>

<p><code>
mongoexport --host HOST --port PORT --db DB_NAME -u USERNAME -p PASSWORD --collection COLLECTION_NANE -q '{}' --out YOUR_FILENAME.json
</code></p>

<h2>Export part of the collection to JSON (using a query)</h2>

<p><code>
mongoexport --host HOST --port PORT --db DB_NAME -u USERNAME -p PASSWORD --collection COLLECTION_NANE -q '{ "some_numeric_field": { "$gte": 100 } }' --out YOUR_FILENAME.json
</code></p>

<p>The most important part here is that the -q options needs to be a valid JSON format query that Mongo knows how to handle <code>{ "some_numeric_field": { "$gte": 100 } }</code>. You can of course use far more complicated queries, but for most cases I don't need to.</p>

<h2>Bonus</h2>

<p>I use <a href="http://kapeli.com/dash">Dash</a> every day, multiple times a day, so it was only natural to have a dash snippet that I can use</p>

<p>Dash snippets are basically a way to paste some code using a shortcode, so typing <code>mongoexport</code> in the console pops up a window where I can complete the rest of the command easily without remembering the options.</p>

<p>Here's the Dash snippets</p>

<p><code>
mongoexport --host __host__ --port __port__ --db __db-name__ -u __username__ -p __pass__ --collection __collection__ -q '__query__' --out __filename__.json
</code></p>

<p>And this is what the window looks like when I type the shortcode, I tab through the place holders and there's a no-brainer way to remember the command and the options.</p>

<p><img src="http://f.cl.ly/items/2x3G2t3N2o2I0817220t/Screenshot_11_22_13__4_56_PM.png" alt="dash snippet window" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Move jobs from one queue to another - Sidekiq]]></title>
    <link href="http://avi.io/blog/2013/08/14/move-jobs-from-one-queue-to-another-sidekiq/"/>
    <updated>2013-08-14T17:36:00+03:00</updated>
    <id>http://avi.io/blog/2013/08/14/move-jobs-from-one-queue-to-another-sidekiq</id>
    <content type="html"><![CDATA[<p>I have been working with Sidekiq for quite a while now, having many jobs per day working (multiple millions of jobs.)</p>

<p>Sometimes, I queue up tasks to a queue called <code>#{queue_name}_pending</code>, I do this so I can manage the load on the servers. (For example: Stop writing to Mongo, Stop importing contact etc...)</p>

<p>This way, I can queue up many jobs, and I can move it to the real queue whenever I feel like it or whenever the problem is solved.</p>

<p>I was looking for a way to move tasks from one queue to another.</p>

<p>There's nothing built into Sidekiq for this, but obviously, you can just use redis built in commands to do it.</p>

<p>Here's the code to do it</p>

<p>```ruby</p>

<pre><code>count_block = proc{ Sidekiq.redis do |conn|
  conn.llen("queue:#{queue_name}")  
end }

while count_block.call &gt; 0
  Sidekiq.redis do |conn|
    conn.rpoplpush "queue:#{queue_name}_pending", "queue:#{queue_name}"
  end
end
</code></pre>

<p>```</p>

<p>This will move all the items from one queue to another until there are no more jobs.</p>

<p>b.t.w
Obviously, the <code>_pending</code> queues don't have any workers assigned to them, the purpose of it is a place holder so the jobs won't go to waste and we can resume work when we can.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[DO NOT use the callbacks that require persistence in Mongoid]]></title>
    <link href="http://avi.io/blog/2013/02/19/do-not-use-the-callbacks-that-require-persistence-in-mongoid/"/>
    <updated>2013-02-19T22:10:00+02:00</updated>
    <id>http://avi.io/blog/2013/02/19/do-not-use-the-callbacks-that-require-persistence-in-mongoid</id>
    <content type="html"><![CDATA[<p>I started using MongoDB at <a href="http://www.gogobot.com">Gogobot</a> a little while ago.
While using it, I encountered some <a href="http://avi.io/blog/2013/01/30/problems-with-mongoid-and-sidekiq-brainstorming/">problems</a>, but for the most part, things went pretty smooth.</p>

<p>Today, I encountered a bug that surprised me.</p>

<p>While it certainly should not have, I think it can surprise you as well, so I am writing it up here as a fair warning.</p>

<p>For Gogobot, the entire graph is built on top of MongoDB, all the things social are driven by it and for the most parts like I mentioned, we are pretty happy with it.</p>

<h2>SO, What was the problem?</h2>

<p>The entire graph is a mountable engine, we can decide to turn it on or to turn it off at will.
It acts as a data warehouse and the workflows are being managed by the app.</p>

<p>For example:</p>

<p>When model X is created, app is notified and decides what to do with this notification, and so on and so forth.</p>

<p>Everything peachy so far, nothing we haven't used hundreds of times in the past.</p>

<p>Here's how it works.</p>

<p>We have a model called <code>VisitedPlace</code>, it's a representation of a user that visited a certain place</p>

<p>Here's the code</p>

<p>```ruby</p>

<pre><code>module GraphEngine
  class FbPlace
    include Mongoid::Document
    include Mongoid::Timestamps
    include GraphEngine::Notifications::NotifiableModel

    #… rest of code here
  end
end
</code></pre>

<p>```</p>

<p>As you can see, this model includes a module called <code>NotifiableModel</code>, here's the important part from it:</p>

<p>```ruby</p>

<pre><code>module GraphEngine
  module Notifications
    module NotifiableModel
      extend ActiveSupport::Concern

      included do
        after_create do
          send_notification("created")
        end
      end

      def send_notification(verb)
        # Notify the app here...
      end
    end
  end
end
</code></pre>

<p>```</p>

<p>Like I said, pretty standard stuff, nothing too fancy, but here's where it's getting tricky.</p>

<p>This model has a unique index on <code>user_id</code> and <code>place_id</code>. It's a unique index and no two documents can exist in the same collection.</p>

<p>BUT… check this out:</p>

<p><code>ruby
  GraphEngine::VisitedPlace.create!(user_id: 1, place_id: 1) =&gt; true
  GraphEngine::VisitedPlace.create!(user_id: 1, place_id: 1) =&gt; true
</code></p>

<p>The second query actually failed in the DB level, but the application still returned true.</p>

<p>Meaning, that <code>after_create</code> is actually being called even if the record is <strong>not really persisted</strong>.</p>

<h2>How you can fix? / should you fix?</h2>

<p>For Gogobot, I fixed it using safe mode on those models, I don't mind the performance penalty, since I don't want to trigger Sidekiq workers that will do all sorts of things twice or three times.</p>

<p>Should you do the same? I am not sure, you need to benchmark your app and see if you can fix it in another way.</p>

<p>Would love to hear back from you in comments/discussion</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Problems with Mongoid and Sidekiq- Brainstorming]]></title>
    <link href="http://avi.io/blog/2013/01/30/problems-with-mongoid-and-sidekiq-brainstorming/"/>
    <updated>2013-01-30T20:13:00+02:00</updated>
    <id>http://avi.io/blog/2013/01/30/problems-with-mongoid-and-sidekiq-brainstorming</id>
    <content type="html"><![CDATA[<p>A few weeks back, we started slowly upgrading all of our queues at <a href="http://www.gogobot.com">Gogobot</a> to work with <a href="https://github.com/mperham/sidekiq">Sidekiq</a>.</p>

<p>Posts on how awesome the experience was and how much better Sidekiq is from <a href="http://github.com/defunkt/sidekiq">Resque</a> coming soon, though with all the good came some bad.</p>

<h2>Summary of the solution</h2>

<p>With Sidekiq, we are processing around <strong>25X more</strong> jobs than what we were doing with Resque, processing around 15,000,000 jobs per day, at paces of over 1K per second at times (at peak we go up well past that)</p>

<p>This is how many jobs we processed today…</p>

<p><img src="http://d.pr/i/O9aU+" alt="Sidekiq history graph for today" /></p>

<p>And this is a snapshot of our realtime graph</p>

<p><img src="http://d.pr/i/7Fkr+" alt="Realtime graph snapshot" /></p>

<p>On the MongoDB side we are working with Mongoid and we have a shared environment, 9 shards with 3 replicas in each shard, all running through 2 routers.</p>

<p>Our production mongoid config looks like this</p>

<p>```yaml
production:
  op_timeout: 3
  connection_timeout: 3
  sessions:</p>

<pre><code>default:
  hosts:
    - HOST_NAME:27017 #Single router R0
  username: USER_NAME
  password: PASSWORD
  database: DATABASE_NAME
  options:
    consistency: :eventual
</code></pre>

<p>```</p>

<p>We are using latest versions of all relevant gems (Sidekiq, Mongoid, Moped, Redis)</p>

<h2>All seems fine right? What's the problem?</h2>

<p>The problem is that we have too many connections opening and closing to our mongo instances. (~25-40 new connections per second).</p>

<p>Each time a job is picked up, a connection to Mongo is opened and when the job is done, this connection is closed (using Kiqstand middleware).</p>

<p>This is causing huge loads on our router server, and causing mongo to run out of file descriptors at times.</p>

<h2>SO?</h2>

<p>More then anything, this post is a callout for discussion with anyone using similar solution with similar scale and can assist, I know I would love to brainstorm on how to solve this problem.</p>
]]></content>
  </entry>
  
</feed>
