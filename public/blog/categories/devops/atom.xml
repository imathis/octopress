<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Devops | Avi Tzurel]]></title>
  <link href="http://avi.io/blog/categories/devops/atom.xml" rel="self"/>
  <link href="http://avi.io/"/>
  <updated>2013-12-03T20:25:19+02:00</updated>
  <id>http://avi.io/</id>
  <author>
    <name><![CDATA[Avi Tzurel]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Upload folder to S3 recursively]]></title>
    <link href="http://avi.io/blog/2013/12/03/upload-folder-to-s3-recursively/"/>
    <updated>2013-12-03T19:46:00+02:00</updated>
    <id>http://avi.io/blog/2013/12/03/upload-folder-to-s3-recursively</id>
    <content type="html"><![CDATA[<p>Lately, I needed to upload a folder to S3 with all of it's files.</p>

<p>The use-case is compiling assets on the CI and then uploading it to S3 for the CDN to consume.</p>

<p>While searching for a gem that does it I encountered <a href="https://github.com/chrishein/s3_uploader">s3_uploader</a>, but I really didn't like it because it's using Fog.</p>

<p>Generally, I don't like gems that use other gems for no apparent reason, there's absolutely no reason to include fog in my project just to upload files recursively.</p>

<p>I did however, like that it's using multi threads in order to do the upload so I am doing the same in my solution.</p>

<p>I wrote a solution that uses the aws-s3 Ruby SDK, which was already included in my project anyway.</p>

<p>Here's the code:</p>

<p>```ruby</p>

<pre><code>class S3FolderUpload
  attr_reader :folder_path, :total_files, :bucket, :s3_connection
  attr_accessor :files

  # Initialize the upload class
  #
  # folder_path - path to the folder that you want to upload
  # bucket - The bucket you want to upload to
  # aws_key - Your key generated by AWS defaults to the environemt setting AWS_KEY_ID
  # aws_secret - The secret generated by AWS
  #
  # Examples
  #   =&gt; uploader = S3FolderUpload.new("some_route/test_folder", 'your_bucket_name')
  #
  def initialize(folder_path, bucket, aws_key = ENV['AWS_KEY_ID'], aws_secret = ENV['AWS_SECRET'])
    @folder_path       = folder_path
    @bucket            = bucket
    @files             = Dir.glob("#{folder_path}/**/*")
    @total_files       = files.length
    @s3_connection     = AWS::S3.new(:access_key_id =&gt; aws_key,
                                    :secret_access_key =&gt; aws_secret)
  end

  # public: Upload files from the folder to S3
  #
  # thread_count - How many threads you want to use (defaults to 5)
  #
  # Examples
  #   =&gt; uploader.upload!(20)
  #     true
  #   =&gt; uploader.upload!
  #     true
  #
  # Returns true when finished the process
  def upload!(thread_count = 5)
    file_number = 0
    mutex       = Mutex.new
    threads     = []

    thread_count.times do |i|
      threads[i] = Thread.new {
        until files.empty?
          mutex.synchronize do
            file_number += 1
            Thread.current["file_number"] = file_number
          end
          file = files.pop rescue nil
          next unless file

          # I had some more manipulation here figuring out the git sha
          # For the sake of the example, we'll leave it simple
          #
          path = file

          puts "[#{Thread.current["file_number"]}/#{total_files}] uploading..."
          data = File.open(file)
          next if File.directory? data

          puts "Path: #{path}"
          obj = s3_connection.buckets[bucket].objects[path]
          obj.write(data, {})
        end
      }
    end
    threads.each { |t| t.join }
  end
end
</code></pre>

<p>```</p>

<p>The usage is really simple</p>

<p>```ruby</p>

<pre><code>uploader = S3FolderUpload.new('folder_name', 'your_bucket', aws_key, aws_secret)
uploader.upload!
</code></pre>

<p>```</p>

<p>Since it's using Threads, the upload is really fast, both from local machines and from servers.</p>

<p>Have fun coding!</p>
]]></content>
  </entry>
  
</feed>
